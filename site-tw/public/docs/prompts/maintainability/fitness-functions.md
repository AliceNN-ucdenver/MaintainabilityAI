# Fitness Functions ‚Äî Maintainability Prompt Pack

> **Fitness Functions** are automated, objective quality gates that continuously validate architectural characteristics. They prevent technical debt by failing builds when code degrades beyond acceptable thresholds.

---

## üéØ What are Fitness Functions?

**Definition**: Executable tests that measure architectural quality metrics (complexity, coverage, performance) and fail if thresholds are exceeded. Think "unit tests for architecture."

**Common Fitness Function Types**:
- **Complexity**: Cyclomatic complexity per function (threshold: ‚â§10)
- **Test Coverage**: Line, branch, and statement coverage (threshold: ‚â•80%)
- **Performance**: p95 latency for critical endpoints (threshold: <200ms)
- **Dependency Freshness**: Age of dependencies (threshold: ‚â§90 days)
- **Security**: High/critical vulnerabilities (threshold: 0)

**Why They Matter**: Without fitness functions, code quality degrades silently over time (architectural erosion). Manual code reviews can't catch every violation.

---

## üîó Maps to OWASP

**Supports**: All OWASP categories by enforcing quality standards
**Primary**: [A06 - Vulnerable Components](/docs/prompts/owasp/A06_vuln_outdated) (dependency freshness)
**Secondary**: [A04 - Insecure Design](/docs/prompts/owasp/A04_insecure_design) (complexity reduces attack surface)

---

## ü§ñ AI Prompt #1: Identify Where to Apply Fitness Functions

<div style="background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%); border-radius: 12px; padding: 24px; margin: 24px 0; border-left: 4px solid #10b981;">

**üìã Copy this prompt and paste it into ChatGPT, Claude, or GitHub Copilot Chat:**

```
Role: You are an Evolutionary Architecture engineer analyzing a codebase to determine which fitness functions would provide the most value.

Context:
I have the following project:

[PASTE YOUR PROJECT DETAILS HERE]

Example:
- Node.js 18 + TypeScript
- 50K+ LOC across 200 files
- Jest test framework
- Express REST API with 30+ endpoints
- PostgreSQL database
- 15 developers contributing
- GitHub Actions CI/CD
- Current issues: high complexity in auth module, inconsistent test coverage, slow dependency updates

Task:
Analyze this project and recommend:

1. Which fitness functions to implement (complexity, coverage, performance, dependencies)
2. Priority order (which will catch the most issues fastest)
3. Baseline thresholds (what limits make sense for THIS codebase, not aspirational goals)
4. Implementation plan (which tools to use, how to integrate with CI)

Format:
For each fitness function, provide:

**Metric**: [What to measure]
**Threshold**: [Acceptable limit based on current state]
**Priority**: [High/Medium/Low]
**Rationale**: [Why this matters for this specific codebase]
**Implementation**: [Which tool/library to use - e.g., ts-complex, autocannon, npm outdated]
**CI Integration**: [How to run in GitHub Actions - be specific]

Focus Areas:
Pay special attention to:
- Hotspot files (high complexity + frequent changes = risk)
- Critical paths (auth, payment, data access)
- Legacy modules (likely candidates for complexity violations)
- Public APIs (need strong test coverage)
- Performance-critical endpoints (user-facing, data-heavy)

Output:
Provide a prioritized list of 3-5 fitness functions with specific thresholds and implementation steps. Start with the fitness function that will catch the most issues with the least effort.
```

</div>

---

## ü§ñ AI Prompt #2: Generate Fitness Function Tests

<div style="background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%); border-radius: 12px; padding: 24px; margin: 24px 0; border-left: 4px solid #10b981;">

**üìã Copy this prompt and paste it into Claude Code, GitHub Copilot Chat, or ChatGPT:**

```
Role: You are a software engineer implementing fitness functions as automated tests that run in CI/CD pipelines.

Context:
I have a Node.js 18 + TypeScript project using Jest for testing and GitHub Actions for CI/CD.

Target metrics:
- Cyclomatic complexity ‚â§10 per function
- Test coverage ‚â•80% (line + branch)
- Dependency age ‚â§90 days
- Performance p95 <200ms for /api/* endpoints

Task: Generate 4 executable fitness function test files:

1. tests/fitness-functions/complexity.test.ts
   - Use ts-complex library to analyze TypeScript files
   - Check cyclomatic complexity for all functions in src/
   - Fail if any function exceeds 10
   - Report violations with file:line:function name
   - Suggest refactoring strategies in error message

2. tests/fitness-functions/coverage.test.ts
   - Read coverage/coverage-summary.json (generated by Jest)
   - Check line, branch, function, statement coverage
   - Fail if any metric <80%
   - Compare against baseline/coverage-baseline.json
   - Fail if coverage dropped >2% from baseline

3. tests/fitness-functions/dependency-freshness.test.ts
   - Run "npm outdated --json" to find old packages
   - Check publish date of each dependency using "npm view <pkg>@<version> time.modified"
   - Fail if any dependency >90 days old
   - Warn if dependency >60 days old
   - Categorize by severity: critical (security), major (breaking), minor (safe)

4. tests/fitness-functions/performance.test.ts
   - Start test server programmatically
   - Use autocannon to load test GET /api/users and POST /api/orders
   - Measure p95, p99 latency and throughput
   - Compare against baseline/perf-baseline.json
   - Fail if p95 >200ms or regressed >10% from baseline
   - Clean up server process after test

Requirements:
- All tests must be Jest .test.ts files that can run with "npm test"
- Tests must fail with actionable error messages (include file paths, actual vs expected values)
- Thresholds should be configurable via environment variables (MAX_COMPLEXITY, MIN_COVERAGE, MAX_DEP_AGE_DAYS)
- Include helper functions for calculations (don't repeat code)
- Add JSDoc comments explaining what each function does

Also generate:
- .github/workflows/fitness-functions.yml (runs on every PR, uploads artifacts)
- baseline/coverage-baseline.json (example structure with 85% coverage)
- baseline/perf-baseline.json (example structure with p95: 145ms)
- README-FITNESS-FUNCTIONS.md (explains how to run tests and update baselines)

Output: Complete, executable TypeScript code for all files. Initially configure CI with continue-on-error: true (warning mode) so we can monitor for 2 weeks before switching to blocking mode.
```

</div>

---

## ‚úÖ Human Review Checklist

After AI generates fitness function tests, **review the code carefully** before running it. Here's what to verify in each area:

<div style="background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%); border-radius: 12px; padding: 28px; margin: 24px 0; border: 1px solid rgba(100, 116, 139, 0.3);">

### üóÇÔ∏è File Structure

Verify the AI created all necessary files in the correct locations. You should have four test files in a fitness-functions directory, a GitHub Actions workflow for CI integration, baseline files for tracking historical metrics, and documentation explaining how to run and update the tests.

---

### üîç Complexity Analysis

The complexity test should use a dedicated tool like `ts-complex` (not manual AST parsing or regex) to analyze your TypeScript code. It needs to correctly count all branching structures including conditionals, loops, case statements, logical operators, and exception handlers. When violations are found, error messages should pinpoint the exact location and suggest specific refactoring patterns like extracting methods or using guard clauses. The threshold should be configurable through environment variables so teams can adjust it based on their needs.

**Test it**: Run the complexity test locally to see which functions exceed the threshold and verify the error messages are actionable.

---

### üìä Coverage Validation

The coverage test should read Jest's coverage report and validate all four metrics: lines, branches, functions, and statements. It needs to compare current coverage against a stored baseline to detect regressions, not just check absolute thresholds. When coverage drops, the error message should clearly show which metric failed and by how much, along with practical remediation steps. Make sure thresholds are configurable so you can start with realistic values and tighten them over time.

**Test it**: Generate a coverage report first, then run the fitness function to verify it correctly identifies gaps.

---

### üì¶ Dependency Freshness

The dependency test should check the actual publish dates of your dependencies, not just compare version numbers. It needs to categorize outdated packages by severity (security issues are more urgent than minor version bumps) and provide clear upgrade paths. The test should warn before it fails, giving teams time to plan upgrades. Make sure it integrates with npm audit to flag known security vulnerabilities in outdated packages.

**Test it**: Run the dependency check to see which packages are flagged and verify the age calculations are accurate.

---

### ‚ö° Performance Testing

The performance test should start your application programmatically, run realistic load tests against critical endpoints, and then cleanly shut down. It needs to measure both absolute latency (p95, p99) and regression from baseline to catch gradual performance degradation. The test must properly clean up spawned processes to avoid leaving servers running in the background. Error messages should show actual vs expected latency with percentage regression for easy interpretation.

**Test it**: Ensure the test starts the server, completes the load test, and terminates cleanly without hanging processes.

---

### ‚öôÔ∏è CI/CD Integration

The GitHub Actions workflow should run on every pull request and push to main, using `npm ci` for deterministic dependency installation. Initially configure it with `continue-on-error: true` so you can monitor results for a couple weeks before switching to blocking mode. The workflow should upload test results as artifacts for historical tracking and optionally comment on pull requests with pass/fail summaries.

**After monitoring period**: Change to blocking mode by setting `continue-on-error: false`.

---

### üìà Baseline Management

Baseline files store historical metrics so you can detect regressions over time. They should contain realistic starting values based on your current codebase, not aspirational goals. These files must be committed to Git and include metadata like timestamp and commit SHA for tracking. Make sure the documentation explains how to regenerate baselines when you intentionally change thresholds or make architectural improvements.

**Update process**: After validating test results, copy fresh metrics to baseline files and commit them with a clear message explaining the change.

---

### üõ°Ô∏è Security Review

Before running any AI-generated code, scan it for security risks. Check for hardcoded secrets, arbitrary code execution patterns like `eval` or unsanitized `exec` calls, external network calls that could leak data, file system operations that go beyond the project directory, and dependencies from unknown sources. AI-generated tests should be self-contained and offline-first.

**Red flags**: Any code that exfiltrates data, executes arbitrary input, or accesses sensitive system resources.

---

### üß™ Final Validation

Install any new dependencies the tests require, then run each fitness function individually to verify it works and produces clear output. Run all tests together to check for conflicts or race conditions. Validate the GitHub Actions workflow syntax before committing it. If you have time, test the full CI pipeline locally using tools like `act` to catch issues before pushing.

**Expected outcome**: Tests should either pass cleanly or fail with specific, actionable error messages that guide you to the problem.

</div>

---

## üîÑ Next Steps

1. **Use Prompt #1** with ChatGPT/Claude to identify which fitness functions your project needs
2. **Use Prompt #2** to generate the test code
3. **Review generated code** using the checklist above
4. **Run tests locally**: `npm test tests/fitness-functions`
5. **Create baselines**: Run tests once, copy results to `baseline/`
6. **Integrate CI**: Add workflow to `.github/workflows/`
7. **Start in warning mode**: Monitor for 2 weeks, then switch to blocking
8. **Monitor trends**: Track metrics over time (Grafana, DataDog)
9. **Refactor violations**: Use [Technical Debt Management](technical-debt) to prioritize fixes

---

## üìñ Additional Resources

- **[Dependency Hygiene Prompt Pack](dependency-hygiene)** ‚Äî Enforce 90-day freshness rule
- **[Technical Debt Management](technical-debt)** ‚Äî Track and prioritize refactoring work
- **Book**: *Building Evolutionary Architectures* (Ford, Parsons, Kua)

---

**Remember**: Fitness functions prevent architectural erosion. Manual reviews catch bugs; fitness functions enforce quality standards automatically.
